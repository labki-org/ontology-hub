---
phase: 01-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docker-compose.yml
  - backend/Dockerfile
  - backend/requirements.txt
  - backend/app/main.py
  - backend/app/config.py
  - backend/app/database.py
  - backend/app/models/__init__.py
  - backend/app/models/entity.py
  - backend/app/models/module.py
  - backend/app/models/draft.py
  - backend/alembic.ini
  - backend/alembic/env.py
  - backend/alembic/versions/001_initial_schema.py
  - .env.example
autonomous: true

must_haves:
  truths:
    - "Developer runs docker compose up and backend + database start successfully"
    - "GET /health returns 200 with database connection confirmed"
    - "Database contains tables for entities, modules, profiles, and drafts"
    - "Alembic migrations can be run to create/update schema"
  artifacts:
    - path: "docker-compose.yml"
      provides: "Multi-container orchestration"
      contains: "backend.*db.*pgadmin.*grafana"
    - path: "backend/app/main.py"
      provides: "FastAPI application with lifespan"
      exports: ["app"]
    - path: "backend/app/database.py"
      provides: "Async database engine and session"
      exports: ["engine", "get_session", "SessionDep"]
    - path: "backend/app/models/entity.py"
      provides: "Entity SQLModel tables"
      contains: "class.*table=True"
    - path: "backend/app/models/draft.py"
      provides: "Draft SQLModel table"
      contains: "class Draft.*table=True"
  key_links:
    - from: "backend/app/main.py"
      to: "backend/app/database.py"
      via: "lifespan context manager"
      pattern: "lifespan.*engine"
    - from: "docker-compose.yml"
      to: "backend/Dockerfile"
      via: "build context"
      pattern: "build:.*backend"
    - from: "backend/app/database.py"
      to: ".env"
      via: "DATABASE_URL config"
      pattern: "postgresql\\+asyncpg://"
---

<objective>
Docker infrastructure and database foundation for Ontology Hub

Purpose: Establish the core development environment with Docker Compose orchestrating FastAPI backend, PostgreSQL database, and observability tools. Create SQLModel database schema for entities (categories, properties, subobjects), modules, profiles, and drafts.

Output: Working `docker compose up` that starts all services, database schema ready for entity storage, health endpoint confirming connectivity.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-CONTEXT.md
@.planning/phases/01-foundation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Docker Compose and Backend Scaffold</name>
  <files>
    docker-compose.yml
    backend/Dockerfile
    backend/requirements.txt
    backend/app/__init__.py
    backend/app/main.py
    backend/app/config.py
    backend/app/database.py
    .env.example
  </files>
  <action>
Create Docker Compose configuration with the following services:

1. **db** (PostgreSQL 17):
   - Image: postgres:17-alpine
   - Volume for data persistence: pgdata:/var/lib/postgresql/data
   - Environment: POSTGRES_USER=ontology, POSTGRES_PASSWORD=${POSTGRES_PASSWORD}, POSTGRES_DB=ontology_hub
   - Health check: pg_isready -U ontology (interval 5s, timeout 5s, retries 5)
   - Port: 5432:5432

2. **backend** (FastAPI):
   - Build from backend/Dockerfile
   - Depends on db with condition: service_healthy
   - Environment: DATABASE_URL=postgresql+asyncpg://ontology:${POSTGRES_PASSWORD}@db:5432/ontology_hub
   - Volumes: ./backend:/app (for hot reload)
   - Port: 8000:8000
   - Command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

3. **pgadmin** (Database UI):
   - Image: dpage/pgadmin4:latest
   - Environment: PGADMIN_DEFAULT_EMAIL, PGADMIN_DEFAULT_PASSWORD from env
   - Port: 5050:80
   - Depends on db

4. **grafana** (Observability):
   - Image: grafana/grafana:latest
   - Port: 3000:3000
   - Volume for data persistence

Create backend/Dockerfile:
- FROM python:3.12-slim
- WORKDIR /app
- Copy requirements.txt and install dependencies
- Copy app code
- Expose 8000

Create backend/requirements.txt with:
- fastapi[standard]>=0.115.0
- sqlmodel>=0.0.22
- asyncpg>=0.30.0
- psycopg2-binary>=2.9.0
- alembic>=1.14.0
- pydantic-settings>=2.7.0
- uvicorn[standard]>=0.34.0

Create backend/app/config.py:
- Use pydantic_settings.BaseSettings for configuration
- DATABASE_URL: str (from environment)
- DEBUG: bool = False
- API_V1_PREFIX: str = "/api/v1"

Create backend/app/database.py:
- create_async_engine with postgresql+asyncpg:// URL
- async_sessionmaker for AsyncSession
- get_session async generator dependency
- SessionDep type alias using Annotated[AsyncSession, Depends(get_session)]

Create backend/app/main.py:
- FastAPI app with lifespan context manager
- On startup: create tables using SQLModel.metadata.create_all (for dev; Alembic for prod)
- On shutdown: dispose engine
- Include /health endpoint returning {"status": "healthy", "database": "connected"}
- Health endpoint must actually query database (SELECT 1)

Create .env.example with all required environment variables (no actual secrets).

Use service name 'db' (not 'localhost') in DATABASE_URL for Docker networking.
Do NOT use deprecated @app.on_event("startup") - use lifespan context manager only.
  </action>
  <verify>
Run: docker compose build && docker compose up -d
Wait for services to be healthy: docker compose ps
Test health endpoint: curl http://localhost:8000/health
Verify response includes {"status": "healthy", "database": "connected"}
  </verify>
  <done>
docker compose up starts backend + database, GET /health returns 200 with database confirmation
  </done>
</task>

<task type="auto">
  <name>Task 2: SQLModel Database Schema</name>
  <files>
    backend/app/models/__init__.py
    backend/app/models/entity.py
    backend/app/models/module.py
    backend/app/models/draft.py
  </files>
  <action>
Create SQLModel table definitions following the multiple model inheritance pattern (Base, Table, Create, Update, Public).

**backend/app/models/entity.py** - Entity types (categories, properties, subobjects):

```python
from enum import Enum
from sqlmodel import SQLModel, Field, Column
from sqlalchemy import JSON
from datetime import datetime
from typing import Optional
import uuid

class EntityType(str, Enum):
    CATEGORY = "category"
    PROPERTY = "property"
    SUBOBJECT = "subobject"

class EntityBase(SQLModel):
    entity_id: str = Field(index=True)  # Schema-defined ID (e.g., "Person", "hasName")
    entity_type: EntityType
    label: str
    description: str | None = None
    schema_definition: dict = Field(default_factory=dict, sa_column=Column(JSON))

class Entity(EntityBase, table=True):
    __tablename__ = "entities"
    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)
    commit_sha: str | None = None  # For versioning from GitHub
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    deleted_at: datetime | None = None  # Soft delete
```

Add EntityCreate, EntityUpdate, EntityPublic schemas.

**backend/app/models/module.py** - Modules and profiles:

```python
class ModuleBase(SQLModel):
    module_id: str = Field(unique=True, index=True)
    label: str
    description: str | None = None
    category_ids: list[str] = Field(default_factory=list, sa_column=Column(JSON))
    dependencies: list[str] = Field(default_factory=list, sa_column=Column(JSON))

class Module(ModuleBase, table=True):
    __tablename__ = "modules"
    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)
    # ... timestamps, soft delete

class ProfileBase(SQLModel):
    profile_id: str = Field(unique=True, index=True)
    label: str
    description: str | None = None
    module_ids: list[str] = Field(default_factory=list, sa_column=Column(JSON))

class Profile(ProfileBase, table=True):
    __tablename__ = "profiles"
    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)
    # ... timestamps, soft delete
```

**backend/app/models/draft.py** - Draft proposals:

```python
class DraftStatus(str, Enum):
    PENDING = "pending"
    VALIDATED = "validated"
    SUBMITTED = "submitted"  # PR created
    EXPIRED = "expired"

class DraftBase(SQLModel):
    status: DraftStatus = DraftStatus.PENDING
    payload: dict = Field(default_factory=dict, sa_column=Column(JSON))
    source_wiki: str | None = None
    base_commit_sha: str | None = None

class Draft(DraftBase, table=True):
    __tablename__ = "drafts"
    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)
    capability_hash: str = Field(unique=True, index=True)  # SHA-256 of token
    expires_at: datetime
    created_at: datetime = Field(default_factory=datetime.utcnow)
    # Note: capability token is NEVER stored, only the hash
```

**backend/app/models/__init__.py**:
- Import and re-export all models
- Ensure all table models are imported before SQLModel.metadata.create_all runs

Use JSONB columns (via Column(JSON) in SQLModel) for flexible schema storage.
All tables use UUID primary keys for security (non-sequential IDs).
Include created_at, updated_at, deleted_at on all tables for auditing and soft deletes.
  </action>
  <verify>
Run: docker compose restart backend
Check logs: docker compose logs backend (should show table creation)
Connect to database: docker compose exec db psql -U ontology -d ontology_hub -c "\dt"
Verify tables exist: entities, modules, profiles, drafts
  </verify>
  <done>
Database contains entities, modules, profiles, drafts tables with correct columns and types
  </done>
</task>

<task type="auto">
  <name>Task 3: Alembic Migration Setup</name>
  <files>
    backend/alembic.ini
    backend/alembic/env.py
    backend/alembic/script.py.mako
    backend/alembic/versions/001_initial_schema.py
  </files>
  <action>
Initialize Alembic for database migrations:

1. Create backend/alembic.ini:
   - Set script_location = alembic
   - Set sqlalchemy.url to use environment variable (will be overridden in env.py)
   - Set file_template for migration naming: %%(year)d%%(month).2d%%(day).2d_%%(hour).2d%%(minute).2d_%%(rev)s_%%(slug)s

2. Create backend/alembic/env.py:
   - Import all SQLModel models from app.models
   - Import SQLModel metadata as target_metadata
   - Use async engine with asyncpg driver
   - Read DATABASE_URL from environment
   - Configure for autogenerate: compare_type=True

Key pattern for async Alembic:
```python
from sqlmodel import SQLModel
from app.models import *  # Import all models
from app.config import settings

target_metadata = SQLModel.metadata

def run_migrations_offline():
    # Offline mode for SQL script generation
    url = settings.DATABASE_URL.replace("+asyncpg", "")  # Alembic needs sync URL
    context.configure(url=url, target_metadata=target_metadata, literal_binds=True)
    with context.begin_transaction():
        context.run_migrations()

def do_run_migrations(connection):
    context.configure(connection=connection, target_metadata=target_metadata)
    with context.begin_transaction():
        context.run_migrations()

async def run_async_migrations():
    connectable = create_async_engine(settings.DATABASE_URL)
    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)
    await connectable.dispose()

def run_migrations_online():
    asyncio.run(run_async_migrations())
```

3. Create initial migration (backend/alembic/versions/001_initial_schema.py):
   - Generate from current models
   - Include all tables: entities, modules, profiles, drafts
   - Include indexes on frequently queried columns

4. Update backend/requirements.txt if not already including alembic.

5. Add migration command to docker-compose.yml or document manual process:
   - docker compose exec backend alembic upgrade head

Do NOT use sync database URL with async models - convert URL for Alembic offline mode.
  </action>
  <verify>
Run migration: docker compose exec backend alembic upgrade head
Check migration status: docker compose exec backend alembic current
Verify output shows revision at head
Test autogenerate: docker compose exec backend alembic revision --autogenerate -m "test" (should show "No changes detected" if schema matches)
  </verify>
  <done>
Alembic migrations configured, initial migration creates all tables, alembic upgrade head succeeds
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Full stack test:
   ```bash
   docker compose down -v  # Clean slate
   docker compose up -d
   docker compose ps  # All services healthy
   curl http://localhost:8000/health  # Returns 200
   ```

2. Database verification:
   ```bash
   docker compose exec db psql -U ontology -d ontology_hub -c "\dt"
   # Shows: entities, modules, profiles, drafts, alembic_version
   ```

3. Migration verification:
   ```bash
   docker compose exec backend alembic current
   # Shows current revision at head
   ```

4. Hot reload test:
   - Modify backend/app/main.py
   - Check docker compose logs backend shows reload
   - Verify change reflected at http://localhost:8000/health
</verification>

<success_criteria>
- docker compose up starts all services (backend, db, pgadmin, grafana)
- Health endpoint confirms database connectivity
- Database schema includes all required tables
- Alembic migrations work for schema changes
- Hot reload works for backend development
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-01-SUMMARY.md`
</output>
