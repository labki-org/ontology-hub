---
phase: 02-github-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/config.py
  - backend/app/services/__init__.py
  - backend/app/services/github.py
  - backend/app/services/indexer.py
  - backend/app/main.py
  - backend/app/models/entity.py
  - backend/alembic/versions/002_entity_unique_constraint.py
  - backend/requirements.txt
autonomous: true

must_haves:
  truths:
    - "Platform can fetch complete repository tree from GitHub API"
    - "Platform can parse entity JSON files from GitHub"
    - "Entities are stored in database with commit SHA"
    - "Rate limits are handled with exponential backoff"
    - "Admin can trigger manual sync via API endpoint and see results"
  artifacts:
    - path: "backend/app/services/github.py"
      provides: "GitHub API client with httpx AsyncClient"
      exports: ["GitHubClient", "GitHubRateLimitError"]
    - path: "backend/app/services/indexer.py"
      provides: "Entity parsing and database upsert logic"
      exports: ["IndexerService", "sync_repository"]
    - path: "backend/app/config.py"
      provides: "GitHub configuration settings"
      contains: "GITHUB_TOKEN"
    - path: "backend/alembic/versions/002_entity_unique_constraint.py"
      provides: "Migration for (entity_id, entity_type) unique constraint"
      contains: "create_unique_constraint"
  key_links:
    - from: "backend/app/services/github.py"
      to: "https://api.github.com"
      via: "httpx AsyncClient with lifespan"
      pattern: "AsyncClient.*base_url.*api.github.com"
    - from: "backend/app/services/indexer.py"
      to: "backend/app/services/github.py"
      via: "GitHubClient import"
      pattern: "from app.services.github import"
    - from: "backend/app/services/indexer.py"
      to: "prisma/entity model"
      via: "upsert with on_conflict_do_update"
      pattern: "on_conflict_do_update"
    - from: "backend/alembic/versions/002_entity_unique_constraint.py"
      to: "entities table"
      via: "unique constraint on (entity_id, entity_type)"
      pattern: "uq_entities_entity_id_type"
---

<objective>
Create GitHub API client and indexer service for fetching and storing entity data from the SemanticSchemas repository.

Purpose: Enable the platform to index the canonical schema repository and store entities in PostgreSQL with version tracking via commit SHA.
Output: Working GitHub client with rate limit handling, indexer service that parses and upserts entities, database migration for unique constraint, and config for GitHub credentials.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 1 established patterns
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/01-foundation/01-02-SUMMARY.md

# Research for this phase
@.planning/phases/02-github-integration/02-RESEARCH.md

# Existing codebase
@backend/app/main.py
@backend/app/config.py
@backend/app/database.py
@backend/app/models/entity.py
@backend/app/models/module.py
@backend/alembic/versions/001_initial_schema.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: GitHub API Client with Lifespan Management</name>
  <files>
    backend/app/config.py
    backend/app/services/__init__.py
    backend/app/services/github.py
    backend/requirements.txt
  </files>
  <action>
**Add to config.py:**
- GITHUB_TOKEN: str (required, from environment)
- GITHUB_REPO_OWNER: str = "labki-org" (or appropriate default)
- GITHUB_REPO_NAME: str = "SemanticSchemas" (or appropriate default)
- GITHUB_WEBHOOK_SECRET: Optional[str] = None

**Add to requirements.txt:**
- tenacity>=9.0.0

**Create backend/app/services/__init__.py:**
- Empty or minimal exports

**Create backend/app/services/github.py:**

1. Define `GitHubRateLimitError` exception with `reset_time: int` attribute

2. Create `GitHubClient` class:
   - `__init__(self, client: httpx.AsyncClient)` - receives pre-configured client
   - Uses tenacity retry decorator for rate limit handling:
     - `@retry(retry=retry_if_exception_type(GitHubRateLimitError), wait=wait_exponential_jitter(initial=1, max=120), stop=stop_after_attempt(5))`
   - `async def _request(self, method: str, url: str, **kwargs) -> dict` - base request method that checks for 403 rate limit responses
   - `async def get_repository_tree(self, owner: str, repo: str, sha: str = "HEAD") -> list[dict]` - uses Git Trees API with `?recursive=1`
   - `async def get_file_content(self, owner: str, repo: str, path: str, ref: str = "main") -> dict` - fetches and base64-decodes file content
   - `async def get_latest_commit_sha(self, owner: str, repo: str, branch: str = "main") -> str` - get HEAD commit SHA

3. Filter tree entries for `.json` files in `categories/`, `properties/`, `subobjects/`, `modules/`, `profiles/` directories

Use patterns from 02-RESEARCH.md - especially the lifespan-managed client pattern (client will be created in main.py lifespan).
  </action>
  <verify>
- `python -c "from app.services.github import GitHubClient, GitHubRateLimitError"` succeeds
- Type hints validate with no errors
- tenacity is in requirements.txt
  </verify>
  <done>
GitHubClient class exists with tree fetching, file content retrieval, and exponential backoff retry logic for rate limits.
  </done>
</task>

<task type="auto">
  <name>Task 2: Database Migration for Entity Unique Constraint</name>
  <files>
    backend/alembic/versions/002_entity_unique_constraint.py
    backend/app/models/entity.py
  </files>
  <action>
**Create backend/alembic/versions/002_entity_unique_constraint.py:**

Create Alembic migration that adds unique constraint on (entity_id, entity_type) combination. This is REQUIRED for the upsert logic to work with `on_conflict_do_update`.

```python
"""Add unique constraint for entity upsert.

Revision ID: 002
Revises: 001
Create Date: 2026-01-21

This constraint enables ON CONFLICT DO UPDATE upserts by (entity_id, entity_type).
"""
from typing import Sequence, Union

from alembic import op

revision: str = "002"
down_revision: Union[str, None] = "001"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # Add unique constraint on (entity_id, entity_type) for upsert support
    op.create_unique_constraint(
        "uq_entities_entity_id_type",
        "entities",
        ["entity_id", "entity_type"]
    )


def downgrade() -> None:
    op.drop_constraint("uq_entities_entity_id_type", "entities", type_="unique")
```

**Update backend/app/models/entity.py:**

Add table args to declare the unique constraint in the SQLModel class (for documentation/introspection):

```python
class Entity(EntityBase, table=True):
    """Entity database table."""

    __tablename__ = "entities"
    __table_args__ = (
        sa.UniqueConstraint("entity_id", "entity_type", name="uq_entities_entity_id_type"),
    )
    # ... rest of fields
```

**Run migration:**
```bash
docker compose exec backend alembic upgrade head
```
  </action>
  <verify>
- Migration file exists at `backend/alembic/versions/002_entity_unique_constraint.py`
- `docker compose exec backend alembic upgrade head` succeeds
- Verify constraint exists: `docker compose exec db psql -U ontology -d ontology_hub -c "\d entities"` shows unique constraint
  </verify>
  <done>
Unique constraint on (entity_id, entity_type) exists in database, enabling safe upsert operations with ON CONFLICT DO UPDATE.
  </done>
</task>

<task type="auto">
  <name>Task 3: Indexer Service with Upsert Logic</name>
  <files>
    backend/app/services/indexer.py
    backend/app/services/__init__.py
  </files>
  <action>
**Create backend/app/services/indexer.py:**

1. Define entity type mapping:
   ```python
   DIRECTORY_TO_TYPE = {
       "categories": EntityType.CATEGORY,
       "properties": EntityType.PROPERTY,
       "subobjects": EntityType.SUBOBJECT,
   }
   ```

2. Create `IndexerService` class:
   - `__init__(self, github_client: GitHubClient, session: AsyncSession)`
   - `async def parse_entity_file(self, content: dict, entity_type: EntityType) -> dict` - extract entity_id, label, description, schema_definition from JSON
   - `async def upsert_entity(self, entity_data: dict, commit_sha: str)` - use PostgreSQL `insert().on_conflict_do_update()` pattern from research, using the unique constraint `uq_entities_entity_id_type`
   - `async def upsert_module(self, content: dict, commit_sha: str)` - parse and upsert module
   - `async def upsert_profile(self, content: dict, commit_sha: str)` - parse and upsert profile

3. Create `async def sync_repository(github_client: GitHubClient, session: AsyncSession, owner: str, repo: str)` function:
   - Get latest commit SHA
   - Get repository tree
   - Filter for entity/module/profile JSON files
   - Fetch content for each file (consider batching or parallelism)
   - Parse and upsert each entity
   - Log progress: files processed, entities upserted, errors skipped
   - Handle parse errors gracefully: skip invalid files, log warning, continue
   - Use transaction: all changes commit together or rollback
   - Return dict with sync stats: `{"commit_sha": str, "entities_synced": int, "files_processed": int, "errors": int}`

4. Add logging for:
   - Sync start/complete with timing
   - Files skipped due to parse errors (log file path and error, NOT content)
   - Entity counts by type

**Update backend/app/services/__init__.py:**
- Export IndexerService, sync_repository, GitHubClient
  </action>
  <verify>
- `python -c "from app.services.indexer import IndexerService, sync_repository"` succeeds
- `python -c "from app.services import GitHubClient, IndexerService, sync_repository"` succeeds
  </verify>
  <done>
IndexerService class exists with entity parsing and PostgreSQL upsert logic. sync_repository function performs full repository sync with transaction safety and returns sync statistics.
  </done>
</task>

<task type="auto">
  <name>Task 4: Integrate GitHub Client into App Lifespan with Sync Endpoint</name>
  <files>
    backend/app/main.py
  </files>
  <action>
**Modify backend/app/main.py lifespan context manager:**

1. After database initialization, create httpx AsyncClient for GitHub:
   ```python
   import httpx
   from app.config import settings

   # Inside lifespan, after database setup:
   timeout = httpx.Timeout(connect=5.0, read=30.0, write=10.0, pool=5.0)
   limits = httpx.Limits(max_connections=10, max_keepalive_connections=5)

   app.state.github_client = httpx.AsyncClient(
       base_url="https://api.github.com",
       timeout=timeout,
       limits=limits,
       headers={
           "Accept": "application/vnd.github+json",
           "X-GitHub-Api-Version": "2022-11-28",
           "Authorization": f"Bearer {settings.GITHUB_TOKEN}",
       }
   )
   ```

2. In shutdown (after yield), close the GitHub client:
   ```python
   await app.state.github_client.aclose()
   ```

3. Add a `/admin/sync` endpoint for manual sync trigger:
   ```python
   from app.services.github import GitHubClient
   from app.services.indexer import sync_repository
   from app.database import get_session

   @app.post("/admin/sync")
   async def trigger_sync(
       request: Request,
       session: AsyncSession = Depends(get_session)
   ):
       """Trigger full repository sync. Returns sync statistics."""
       github_client = GitHubClient(request.app.state.github_client)
       result = await sync_repository(
           github_client=github_client,
           session=session,
           owner=settings.GITHUB_REPO_OWNER,
           repo=settings.GITHUB_REPO_NAME
       )
       return result
   ```

4. Create dependency for getting GitHubClient from app state:
   ```python
   from fastapi import Request

   def get_github_client(request: Request) -> GitHubClient:
       return GitHubClient(request.app.state.github_client)
   ```

**Update .env.example:**
- Add GITHUB_TOKEN=your_github_token_here
- Add GITHUB_REPO_OWNER=labki-org (optional with default)
- Add GITHUB_REPO_NAME=SemanticSchemas (optional with default)
  </action>
  <verify>
**Basic verification:**
- Docker containers rebuild: `docker compose build backend`
- Application starts without errors (assuming GITHUB_TOKEN is set or made optional for startup)
- `/admin/sync` endpoint exists in OpenAPI docs at http://localhost:8080/docs

**Functional verification (requires valid GITHUB_TOKEN):**
- Call sync endpoint: `curl -X POST http://localhost:8080/admin/sync`
- Response contains sync stats: `{"commit_sha": "...", "entities_synced": N, "files_processed": N, "errors": N}`
- Entities appear in database: `docker compose exec db psql -U ontology -d ontology_hub -c "SELECT COUNT(*), entity_type FROM entities GROUP BY entity_type;"`
- If no GITHUB_TOKEN configured, endpoint returns appropriate error (401/500 with message)
  </verify>
  <done>
GitHub client is created at app startup and properly closed at shutdown. Manual sync endpoint available and tested - calling POST /admin/sync triggers repository indexing and entities appear in database with correct commit_sha.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **GitHub client works:**
   - With valid GITHUB_TOKEN, can fetch public repository tree
   - Rate limit retry logic is in place (testable by mocking 403 response)

2. **Database supports upsert:**
   - Unique constraint on (entity_id, entity_type) exists
   - Migration applied successfully

3. **Indexer processes files:**
   - Given repository tree and file contents, creates/updates entities in database
   - Transaction ensures atomicity

4. **Integration works end-to-end:**
   - POST /admin/sync triggers full repository sync
   - Response includes sync statistics
   - Entities appear in database with correct commit_sha
   - Verify with: `docker compose exec db psql -U ontology -d ontology_hub -c "SELECT COUNT(*), entity_type FROM entities GROUP BY entity_type;"`
</verification>

<success_criteria>
- GitHub client fetches repository tree and file contents
- Unique constraint on (entity_id, entity_type) exists in database
- Indexer parses entity JSON and upserts to database
- Manual sync endpoint triggers full repository indexing
- Sync endpoint returns statistics (commit_sha, entities_synced, files_processed, errors)
- Entities in database have commit_sha populated
- Parse errors are logged but don't block sync
- Rate limits trigger exponential backoff retry
</success_criteria>

<output>
After completion, create `.planning/phases/02-github-integration/02-01-SUMMARY.md`
</output>
