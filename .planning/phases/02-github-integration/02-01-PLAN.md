---
phase: 02-github-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/config.py
  - backend/app/services/__init__.py
  - backend/app/services/github.py
  - backend/app/services/indexer.py
  - backend/app/main.py
  - backend/requirements.txt
autonomous: true

must_haves:
  truths:
    - "Platform can fetch complete repository tree from GitHub API"
    - "Platform can parse entity JSON files from GitHub"
    - "Entities are stored in database with commit SHA"
    - "Rate limits are handled with exponential backoff"
  artifacts:
    - path: "backend/app/services/github.py"
      provides: "GitHub API client with httpx AsyncClient"
      exports: ["GitHubClient", "GitHubRateLimitError"]
    - path: "backend/app/services/indexer.py"
      provides: "Entity parsing and database upsert logic"
      exports: ["IndexerService", "sync_repository"]
    - path: "backend/app/config.py"
      provides: "GitHub configuration settings"
      contains: "GITHUB_TOKEN"
  key_links:
    - from: "backend/app/services/github.py"
      to: "https://api.github.com"
      via: "httpx AsyncClient with lifespan"
      pattern: "AsyncClient.*base_url.*api.github.com"
    - from: "backend/app/services/indexer.py"
      to: "backend/app/services/github.py"
      via: "GitHubClient import"
      pattern: "from app.services.github import"
    - from: "backend/app/services/indexer.py"
      to: "prisma/entity model"
      via: "upsert with on_conflict_do_update"
      pattern: "on_conflict_do_update"
---

<objective>
Create GitHub API client and indexer service for fetching and storing entity data from the SemanticSchemas repository.

Purpose: Enable the platform to index the canonical schema repository and store entities in PostgreSQL with version tracking via commit SHA.
Output: Working GitHub client with rate limit handling, indexer service that parses and upserts entities, and config for GitHub credentials.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 1 established patterns
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/01-foundation/01-02-SUMMARY.md

# Research for this phase
@.planning/phases/02-github-integration/02-RESEARCH.md

# Existing codebase
@backend/app/main.py
@backend/app/config.py
@backend/app/database.py
@backend/app/models/entity.py
@backend/app/models/module.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: GitHub API Client with Lifespan Management</name>
  <files>
    backend/app/config.py
    backend/app/services/__init__.py
    backend/app/services/github.py
    backend/requirements.txt
  </files>
  <action>
**Add to config.py:**
- GITHUB_TOKEN: str (required, from environment)
- GITHUB_REPO_OWNER: str = "labki-org" (or appropriate default)
- GITHUB_REPO_NAME: str = "SemanticSchemas" (or appropriate default)
- GITHUB_WEBHOOK_SECRET: Optional[str] = None

**Add to requirements.txt:**
- tenacity>=9.0.0

**Create backend/app/services/__init__.py:**
- Empty or minimal exports

**Create backend/app/services/github.py:**

1. Define `GitHubRateLimitError` exception with `reset_time: int` attribute

2. Create `GitHubClient` class:
   - `__init__(self, client: httpx.AsyncClient)` - receives pre-configured client
   - Uses tenacity retry decorator for rate limit handling:
     - `@retry(retry=retry_if_exception_type(GitHubRateLimitError), wait=wait_exponential_jitter(initial=1, max=120), stop=stop_after_attempt(5))`
   - `async def _request(self, method: str, url: str, **kwargs) -> dict` - base request method that checks for 403 rate limit responses
   - `async def get_repository_tree(self, owner: str, repo: str, sha: str = "HEAD") -> list[dict]` - uses Git Trees API with `?recursive=1`
   - `async def get_file_content(self, owner: str, repo: str, path: str, ref: str = "main") -> dict` - fetches and base64-decodes file content
   - `async def get_latest_commit_sha(self, owner: str, repo: str, branch: str = "main") -> str` - get HEAD commit SHA

3. Filter tree entries for `.json` files in `categories/`, `properties/`, `subobjects/`, `modules/`, `profiles/` directories

Use patterns from 02-RESEARCH.md - especially the lifespan-managed client pattern (client will be created in main.py lifespan).
  </action>
  <verify>
- `python -c "from app.services.github import GitHubClient, GitHubRateLimitError"` succeeds
- Type hints validate with no errors
- tenacity is in requirements.txt
  </verify>
  <done>
GitHubClient class exists with tree fetching, file content retrieval, and exponential backoff retry logic for rate limits.
  </done>
</task>

<task type="auto">
  <name>Task 2: Indexer Service with Upsert Logic</name>
  <files>
    backend/app/services/indexer.py
    backend/app/services/__init__.py
  </files>
  <action>
**Create backend/app/services/indexer.py:**

1. Define entity type mapping:
   ```python
   DIRECTORY_TO_TYPE = {
       "categories": EntityType.CATEGORY,
       "properties": EntityType.PROPERTY,
       "subobjects": EntityType.SUBOBJECT,
   }
   ```

2. Create `IndexerService` class:
   - `__init__(self, github_client: GitHubClient, session: AsyncSession)`
   - `async def parse_entity_file(self, content: dict, entity_type: EntityType) -> dict` - extract entity_id, label, description, schema_definition from JSON
   - `async def upsert_entity(self, entity_data: dict, commit_sha: str)` - use PostgreSQL `insert().on_conflict_do_update()` pattern from research
   - `async def upsert_module(self, content: dict, commit_sha: str)` - parse and upsert module
   - `async def upsert_profile(self, content: dict, commit_sha: str)` - parse and upsert profile

3. Create `async def sync_repository(github_client: GitHubClient, session: AsyncSession, owner: str, repo: str)` function:
   - Get latest commit SHA
   - Get repository tree
   - Filter for entity/module/profile JSON files
   - Fetch content for each file (consider batching or parallelism)
   - Parse and upsert each entity
   - Log progress: files processed, entities upserted, errors skipped
   - Handle parse errors gracefully: skip invalid files, log warning, continue
   - Use transaction: all changes commit together or rollback

4. Add logging for:
   - Sync start/complete with timing
   - Files skipped due to parse errors (log file path and error, NOT content)
   - Entity counts by type

**Update backend/app/services/__init__.py:**
- Export IndexerService, sync_repository, GitHubClient
  </action>
  <verify>
- `python -c "from app.services.indexer import IndexerService, sync_repository"` succeeds
- `python -c "from app.services import GitHubClient, IndexerService, sync_repository"` succeeds
  </verify>
  <done>
IndexerService class exists with entity parsing and PostgreSQL upsert logic. sync_repository function performs full repository sync with transaction safety.
  </done>
</task>

<task type="auto">
  <name>Task 3: Integrate GitHub Client into App Lifespan</name>
  <files>
    backend/app/main.py
  </files>
  <action>
**Modify backend/app/main.py lifespan context manager:**

1. After database initialization, create httpx AsyncClient for GitHub:
   ```python
   import httpx
   from app.config import settings

   # Inside lifespan, after database setup:
   timeout = httpx.Timeout(connect=5.0, read=30.0, write=10.0, pool=5.0)
   limits = httpx.Limits(max_connections=10, max_keepalive_connections=5)

   app.state.github_client = httpx.AsyncClient(
       base_url="https://api.github.com",
       timeout=timeout,
       limits=limits,
       headers={
           "Accept": "application/vnd.github+json",
           "X-GitHub-Api-Version": "2022-11-28",
           "Authorization": f"Bearer {settings.GITHUB_TOKEN}",
       }
   )
   ```

2. In shutdown (after yield), close the GitHub client:
   ```python
   await app.state.github_client.aclose()
   ```

3. Add a `/admin/sync` endpoint (or management command) for manual sync trigger:
   - POST /admin/sync - triggers full repository sync
   - Returns sync status (files processed, entities upserted, errors)
   - Consider: should this be rate-limited or admin-protected? For now, leave unprotected for development

4. Create dependency for getting GitHubClient from app state:
   ```python
   from fastapi import Request

   def get_github_client(request: Request) -> GitHubClient:
       return GitHubClient(request.app.state.github_client)
   ```

**Update .env.example:**
- Add GITHUB_TOKEN=your_github_token_here
- Add GITHUB_REPO_OWNER=labki-org (optional with default)
- Add GITHUB_REPO_NAME=SemanticSchemas (optional with default)
  </action>
  <verify>
- Docker containers rebuild: `docker compose build backend`
- Application starts without errors (assuming GITHUB_TOKEN is set or made optional for startup)
- `/admin/sync` endpoint exists in OpenAPI docs at http://localhost:8080/docs
  </verify>
  <done>
GitHub client is created at app startup and properly closed at shutdown. Manual sync endpoint available for testing indexer.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **GitHub client works:**
   - With valid GITHUB_TOKEN, can fetch public repository tree
   - Rate limit retry logic is in place (testable by mocking 403 response)

2. **Indexer processes files:**
   - Given repository tree and file contents, creates/updates entities in database
   - Transaction ensures atomicity

3. **Integration works:**
   - POST /admin/sync triggers full repository sync
   - Entities appear in database with correct commit_sha
   - Verify with: `docker compose exec db psql -U ontology -d ontology_hub -c "SELECT COUNT(*), entity_type FROM entities GROUP BY entity_type;"`
</verification>

<success_criteria>
- GitHub client fetches repository tree and file contents
- Indexer parses entity JSON and upserts to database
- Manual sync endpoint triggers full repository indexing
- Entities in database have commit_sha populated
- Parse errors are logged but don't block sync
- Rate limits trigger exponential backoff retry
</success_criteria>

<output>
After completion, create `.planning/phases/02-github-integration/02-01-SUMMARY.md`
</output>
