---
phase: 24-database-schema
plan: 02
type: execute
wave: 2
depends_on: ["24-01"]
files_modified:
  - backend/alembic/versions/002_dashboard_resource.py
autonomous: true

must_haves:
  truths:
    - "Database tables dashboards and resources exist after migration"
    - "Junction tables module_dashboard and bundle_dashboard exist"
    - "Indexes created on entity_key, label, and category_key columns"
    - "Database reset and ingest completes without errors"
  artifacts:
    - path: "backend/alembic/versions/002_dashboard_resource.py"
      provides: "Alembic migration for Dashboard/Resource tables"
      contains: "def upgrade"
  key_links:
    - from: "backend/alembic/versions/002_dashboard_resource.py"
      to: "dashboards table"
      via: "op.create_table"
      pattern: "create_table.*dashboards"
    - from: "backend/alembic/versions/002_dashboard_resource.py"
      to: "resources table"
      via: "op.create_table"
      pattern: "create_table.*resources"
---

<objective>
Create Alembic migration to add Dashboard and Resource tables with their relationship tables, then verify database reset and ingest.

Purpose: Apply the database schema changes that enable storing dashboard and resource entities. This completes Phase 24's goal of database schema readiness.

Output: Migration file 002_dashboard_resource.py; verified database with new tables.
</objective>

<execution_context>
@/home/daharoni/.claude/get-shit-done/workflows/execute-plan.md
@/home/daharoni/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/24-database-schema/24-CONTEXT.md
@.planning/phases/24-database-schema/24-RESEARCH.md

# Previous plan context
@.planning/phases/24-database-schema/24-01-SUMMARY.md

# Migration pattern to follow
@backend/alembic/versions/001_full_schema.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Alembic migration</name>
  <files>backend/alembic/versions/002_dashboard_resource.py</files>
  <action>
Create migration file following the pattern from 001_full_schema.py:

```python
"""Add Dashboard and Resource tables with relationship tables.

Revision ID: 002
Revises: 001
Create Date: 2026-01-27

Creates:
- dashboards: Dashboard entity table with canonical_json for pages
- resources: Resource entity table with category_key string column
- module_dashboard: Junction table linking modules to dashboards
- bundle_dashboard: Junction table linking bundles to dashboards
"""

from collections.abc import Sequence

import sqlalchemy as sa

from alembic import op

revision: str = "002"
down_revision: str = "001"
branch_labels: str | Sequence[str] | None = None
depends_on: str | Sequence[str] | None = None


def upgrade() -> None:
    # === Entity Tables ===

    # dashboards table
    op.create_table(
        "dashboards",
        sa.Column("id", sa.UUID(), nullable=False),
        sa.Column("entity_key", sa.String(), nullable=False),
        sa.Column("source_path", sa.String(), nullable=False),
        sa.Column("label", sa.String(), nullable=False),
        sa.Column("description", sa.String(), nullable=True),
        sa.Column("canonical_json", sa.JSON(), nullable=True),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("updated_at", sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("entity_key", name="uq_dashboards_entity_key"),
    )
    op.create_index(op.f("ix_dashboards_entity_key"), "dashboards", ["entity_key"])
    op.create_index(op.f("ix_dashboards_label"), "dashboards", ["label"])

    # resources table
    op.create_table(
        "resources",
        sa.Column("id", sa.UUID(), nullable=False),
        sa.Column("entity_key", sa.String(), nullable=False),
        sa.Column("source_path", sa.String(), nullable=False),
        sa.Column("label", sa.String(), nullable=False),
        sa.Column("description", sa.String(), nullable=True),
        sa.Column("category_key", sa.String(), nullable=False),
        sa.Column("canonical_json", sa.JSON(), nullable=True),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("updated_at", sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("entity_key", name="uq_resources_entity_key"),
    )
    op.create_index(op.f("ix_resources_entity_key"), "resources", ["entity_key"])
    op.create_index(op.f("ix_resources_label"), "resources", ["label"])
    op.create_index(op.f("ix_resources_category_key"), "resources", ["category_key"])

    # === Relationship Tables ===

    # module_dashboard - module references dashboard
    op.create_table(
        "module_dashboard",
        sa.Column("module_id", sa.UUID(), nullable=False),
        sa.Column("dashboard_id", sa.UUID(), nullable=False),
        sa.PrimaryKeyConstraint("module_id", "dashboard_id"),
        sa.ForeignKeyConstraint(["module_id"], ["modules_v2.id"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(["dashboard_id"], ["dashboards.id"], ondelete="RESTRICT"),
    )

    # bundle_dashboard - bundle references dashboard
    op.create_table(
        "bundle_dashboard",
        sa.Column("bundle_id", sa.UUID(), nullable=False),
        sa.Column("dashboard_id", sa.UUID(), nullable=False),
        sa.PrimaryKeyConstraint("bundle_id", "dashboard_id"),
        sa.ForeignKeyConstraint(["bundle_id"], ["bundles.id"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(["dashboard_id"], ["dashboards.id"], ondelete="RESTRICT"),
    )


def downgrade() -> None:
    # Drop relationship tables first (FK dependencies)
    op.drop_table("bundle_dashboard")
    op.drop_table("module_dashboard")

    # Drop entity tables
    op.drop_index(op.f("ix_resources_category_key"), table_name="resources")
    op.drop_index(op.f("ix_resources_label"), table_name="resources")
    op.drop_index(op.f("ix_resources_entity_key"), table_name="resources")
    op.drop_table("resources")

    op.drop_index(op.f("ix_dashboards_label"), table_name="dashboards")
    op.drop_index(op.f("ix_dashboards_entity_key"), table_name="dashboards")
    op.drop_table("dashboards")
```

Key decisions per CONTEXT.md:
- category_key is plain String, NOT a foreign key (orphaned resources allowed)
- dashboards table uses ondelete="RESTRICT" (cannot delete dashboard if referenced)
- module_dashboard/bundle_dashboard use ondelete="CASCADE" on module/bundle side
  </action>
  <verify>
ls -la backend/alembic/versions/002_dashboard_resource.py
python -c "import backend.alembic.versions.002_dashboard_resource as m; print('Migration syntax OK')"
  </verify>
  <done>Migration file exists with upgrade() and downgrade() functions</done>
</task>

<task type="auto">
  <name>Task 2: Reset database and run migrations</name>
  <files></files>
  <action>
Reset the database and apply all migrations to verify schema:

1. Stop any running containers
2. Remove database volume to start fresh
3. Start database container
4. Run alembic upgrade head
5. Verify tables exist

Commands (from project root):
```bash
# Stop and remove existing containers/volumes
docker compose down -v

# Start fresh database
docker compose up -d db

# Wait for database to be ready
sleep 3

# Run migrations
cd backend && alembic upgrade head

# Verify tables exist
docker compose exec db psql -U ontology -d ontology_hub -c "\dt"
```

Expected output should show: dashboards, resources, module_dashboard, bundle_dashboard tables among others.
  </action>
  <verify>
docker compose exec db psql -U ontology -d ontology_hub -c "\dt" | grep -E "(dashboards|resources|module_dashboard|bundle_dashboard)"
  </verify>
  <done>All four new tables appear in database table listing</done>
</task>

<task type="auto">
  <name>Task 3: Verify table structure and constraints</name>
  <files></files>
  <action>
Verify the table structures match requirements:

```bash
# Check dashboards table structure
docker compose exec db psql -U ontology -d ontology_hub -c "\d dashboards"

# Check resources table structure
docker compose exec db psql -U ontology -d ontology_hub -c "\d resources"

# Check junction tables
docker compose exec db psql -U ontology -d ontology_hub -c "\d module_dashboard"
docker compose exec db psql -U ontology -d ontology_hub -c "\d bundle_dashboard"

# Verify indexes exist
docker compose exec db psql -U ontology -d ontology_hub -c "\di" | grep -E "(dashboards|resources)"
```

Expected:
- dashboards: id, entity_key, source_path, label, description, canonical_json, created_at, updated_at
- resources: id, entity_key, source_path, label, description, category_key, canonical_json, created_at, updated_at
- module_dashboard: module_id, dashboard_id with FKs
- bundle_dashboard: bundle_id, dashboard_id with FKs
- Indexes on entity_key, label, category_key columns
  </action>
  <verify>
# Verify dashboards has expected columns
docker compose exec db psql -U ontology -d ontology_hub -c "SELECT column_name FROM information_schema.columns WHERE table_name='dashboards'" | grep -E "(entity_key|canonical_json|label)"

# Verify resources has category_key
docker compose exec db psql -U ontology -d ontology_hub -c "SELECT column_name FROM information_schema.columns WHERE table_name='resources'" | grep category_key

# Verify indexes
docker compose exec db psql -U ontology -d ontology_hub -c "SELECT indexname FROM pg_indexes WHERE tablename IN ('dashboards', 'resources')"
  </verify>
  <done>Table structures match design; indexes created on queryable columns</done>
</task>

</tasks>

<verification>
Complete verification from project root:

```bash
cd /home/daharoni/dev/ontology-hub

# 1. Verify migration file exists and is valid Python
python -c "
import importlib.util
spec = importlib.util.spec_from_file_location('m', 'backend/alembic/versions/002_dashboard_resource.py')
m = importlib.util.module_from_spec(spec)
spec.loader.exec_module(m)
assert hasattr(m, 'upgrade')
assert hasattr(m, 'downgrade')
print('Migration valid')
"

# 2. Verify all tables exist
docker compose exec db psql -U ontology -d ontology_hub -c "
SELECT table_name FROM information_schema.tables
WHERE table_schema='public'
AND table_name IN ('dashboards', 'resources', 'module_dashboard', 'bundle_dashboard')
ORDER BY table_name;
"

# 3. Verify foreign key constraints on junction tables
docker compose exec db psql -U ontology -d ontology_hub -c "
SELECT conname, confdeltype
FROM pg_constraint
WHERE conrelid IN ('module_dashboard'::regclass, 'bundle_dashboard'::regclass)
AND contype = 'f';
"
# Should show CASCADE and RESTRICT delete types

# 4. Quick insert/delete test
docker compose exec db psql -U ontology -d ontology_hub -c "
INSERT INTO dashboards (id, entity_key, source_path, label, canonical_json, created_at, updated_at)
VALUES (gen_random_uuid(), 'Test_dashboard', 'dashboards/Test_dashboard.json', 'Test', '{}', NOW(), NOW());
DELETE FROM dashboards WHERE entity_key = 'Test_dashboard';
SELECT 'Insert/delete OK' as result;
"
```
</verification>

<success_criteria>
1. Migration file 002_dashboard_resource.py exists in backend/alembic/versions/
2. `alembic upgrade head` completes without errors
3. dashboards table exists with entity_key, canonical_json, label columns
4. resources table exists with entity_key, category_key, canonical_json columns
5. module_dashboard and bundle_dashboard junction tables exist
6. Foreign key constraints use correct ondelete behavior (CASCADE/RESTRICT)
7. Indexes exist on entity_key, label, and category_key columns
</success_criteria>

<output>
After completion, create `.planning/phases/24-database-schema/24-02-SUMMARY.md`
</output>
